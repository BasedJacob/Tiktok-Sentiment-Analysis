{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting TikTok Sentiment Analysis Based on Google Play Store Reviews\n",
    "### Group: Phishers\n",
    "### Jacob He, Justin Nakatsu, Rebekah Wong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "In order to run our code on the TikTok dataset, please download the following input files first:\n",
    "https://vault.sfu.ca/index.php/s/8F2V1j01WcERKyQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running the code, after downloading the data and the source code, in the source directory set up the virtual environment with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 -m venv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with the dataset in an input folder you can run any of stats.py, bertbase.py, bertTraining.py, lexicalBase.py, lexicalBootstrap.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our BERT tests were adapted following Ataie's (2022)'s paper and the bootstrapping algorithm was coded following the algorithm from Volkova et al.'s paper from 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats.py\n",
    "In the stats.py file we first do some analysis on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "filepath = Path('../input/reviews_evened.csv')  \n",
    "df = pd.read_csv('../input/tiktok_google_play_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read our csv data file from an input folder and will output another data file to the same folder later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(rating):\n",
    "    rating = int(rating)\n",
    "    \n",
    "    # Convert 5 star scale rating to sentiment\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset has the ratings on a 5 star scale, but we only want to determine if a review is positive or negative we convert it to 0 for negative, 1 for neutral, and 2 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df.score.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_posneg(sentiment):\n",
    "    sentiment = int(sentiment)\n",
    "    \n",
    "    # Convert the sentiment to readable class names\n",
    "    if sentiment == 0:\n",
    "        return \"negative\"\n",
    "    elif sentiment == 1:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a sentiment column for faster comparisin and a class column to make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  content     class  sentiment\n",
      "114431                                                 Hh  positive          2\n",
      "265589                                        Very niceee  positive          2\n",
      "35846   I love everything about TicTok except for the ...  positive          2\n",
      "79216                                     Just phenomenal  positive          2\n",
      "166049                                               Nice  positive          2\n",
      "47727                                I really like Tiktok  positive          2\n",
      "181759  I am happy too this apps use because my tiktok...  positive          2\n",
      "33278                                                Good  positive          2\n",
      "123126                                               Good  negative          0\n",
      "267109  Tiktok is a nice place where you do anything t...  negative          0\n",
      "154592                                       Tiktok Thank  positive          2\n",
      "50060                                  Good app I love it  positive          2\n",
      "127579                                Report problem 😭😭😭😭  negative          0\n",
      "114086                                               This  negative          0\n",
      "189247                                          Very nice   neutral          1\n",
      "190973                                      برنامجج قخزيي  positive          2\n",
      "67676                                                Nice  positive          2\n",
      "60729   Çok iyi bir uygulama ben roblox çekiyorum bu d...  positive          2\n",
      "115052  This app very good i liked but my video no vir...  positive          2\n",
      "227688                                    It's wonderfull  positive          2\n"
     ]
    }
   ],
   "source": [
    "df['class'] = df.sentiment.apply(to_posneg)\n",
    "print(df[[\"content\",\"class\",\"sentiment\"]].sample(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecount = [0,0,0,0,0]\n",
    "def count_rating(rating):\n",
    "    rating = int(rating)\n",
    "    scorecount[rating-1] = scorecount[rating-1] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get some data about how many reviews there are for each star rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.score.apply(count_rating)\n",
    "total = sum(scorecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then print out the counts of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating \t|     count\t|     bar\n",
      "\t|\t\t|\n",
      "  1\t|     74160\t|  -----------------\n",
      "\t|\t\t|\n",
      "  2\t|     15666\t|  ----\n",
      "\t|\t\t|\n",
      "  3\t|     23304\t|  ------\n",
      "\t|\t\t|\n",
      "  4\t|     35970\t|  ---------\n",
      "\t|\t\t|\n",
      "  5\t|     465014\t|  -----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cap = max(scorecount)                                               #visualization of rating counts from the entire dataset\n",
    "print(\"rating \\t|     count\\t|     bar\")\n",
    "for i in range(5):\n",
    "    print(\"\\t|\\t\\t|\")\n",
    "    num = 100 * scorecount[i]/cap\n",
    "    num = round(num) \n",
    "    out = \"  -\"\n",
    "    for j in range(num):\n",
    "        out = out + \"-\"\n",
    "    print(\"  \"+str(i+1)+\"\\t|     \"+str(scorecount[i]) + \"\\t|\"+ out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can see the counts of each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of positive = 751476, neutral = 34956, negative = 134739\n",
      "distribution is 81.57833887519256% positive 3.7947351794617936% neutral 14.626925945345652% negative\n",
      "negative/positive = 0.17929913929386967\n"
     ]
    }
   ],
   "source": [
    "negative = scorecount[0] + scorecount[1]\n",
    "neutral = scorecount[2]\n",
    "positive = scorecount[3] + scorecount [4]\n",
    "print(f'counts of positive = {positive}, neutral = {neutral}, negative = {negative}')\n",
    "print(f'distribution is {100*positive/total}% positive {100*neutral/total}% neutral {100*negative/total}% negative')\n",
    "print(f'negative/positive = {negative/positive}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is heavily skewed to positive reviews which helped the BERT model more than our LBA model we can adjust it to be more even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['sentiment'] == 2].sample(frac=positive/total).index)      #remove some random positive reviews to have an even distribution for positive and negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now counting again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of positive = 46145, neutral = 11652, negative = 44913\n",
      "distribution is 44.9274656800701% positive 11.34456236004284% neutral 43.727971959887064% negative\n",
      "negative/positive = 0.9733015494636472\n"
     ]
    }
   ],
   "source": [
    "scorecount = [0,0,0,0,0]\n",
    "df.score.apply(count_rating)\n",
    "total = sum(scorecount)\n",
    "negative = scorecount[0] + scorecount[1]\n",
    "neutral = scorecount[2]\n",
    "positive = scorecount[3] + scorecount [4]\n",
    "print(f'counts of positive = {positive}, neutral = {neutral}, negative = {negative}')\n",
    "print(f'distribution is {100*positive/total}% positive {100*neutral/total}% neutral {100*negative/total}% negative')\n",
    "print(f'negative/positive = {negative/positive}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is much more even. Then we can save the evened out data set to \"../input/reviews_evened.csv\" if we want.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath.parent.mkdir(parents=True, exist_ok=True)  #save evened distribution to a file\n",
    "# df.to_csv(filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lexicalBase.py\n",
    "In lexicalBase.py we do a basic lexical based approach to sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some additional imports and get the VADER lexicon and put it in the \"input\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to ../input...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import nltk\n",
    "nltk.download('vader_lexicon',download_dir=\"../input\")\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the evened dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/reviews_evened.csv')\n",
    "# df = pd.read_csv('../input/tiktok_google_play_reviews.csv')\n",
    "# df = df.sample(n=1245) #cut dataset down for testing\n",
    "\n",
    "df = df.dropna(subset='content',axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_score(sentence):\n",
    "    score = analyser.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenize our reviews with the nltk tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_descriptions = df['content'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the polarity with VADER and convert it into a sentiment value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores'] = df['content'].apply(lambda review: analyser.polarity_scores(review))\n",
    "df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "def Sentimnt(x):\n",
    "    if x>= 0.01:\n",
    "        return \"positive\"\n",
    "    elif x<= -0.01:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "df['Sentiment'] = df['compound'].apply(Sentimnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then see how well it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content     class Sentiment\n",
      "99561  Good but tik tok banned me for good for no reason  negative  negative\n",
      "71594                         How to earning on tik tok?  negative   neutral\n",
      "72879                                Follow back problem  negative  negative\n",
      "17023  Its is very entertaining.it consumes alot of d...   neutral  positive\n",
      "15133                                               op,,  positive   neutral\n",
      "10101                                                Sei  positive   neutral\n",
      "46729                                     mohmamad faydu  negative   neutral\n",
      "7902   Sharing Ticktoks doesn't work. When I get them...  negative  positive\n",
      "64416  Not good because my account is freeze or not v...  negative  negative\n",
      "28278                                              Great  positive  positive\n",
      "35516  So I post videos and and I let my inbox get 99...  negative  negative\n",
      "41381                                               Good  positive  positive\n",
      "20733  It's the best app that I see in my life Pls he...  positive  positive\n",
      "64465                                 .This App Dowlodes  positive   neutral\n",
      "62484                           Very good ap so nice app  negative  positive\n",
      "38116                                     Ilove this app  positive   neutral\n",
      "17255                   Please tik tok my video viral🥺🥺🥺  positive  positive\n",
      "64888                             The scurge of society.  negative   neutral\n",
      "66901                                     Very nice app😍   neutral  positive\n",
      "29598       I will rate 5star if my viewers can increase  negative  positive\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"content\",\"class\",\"Sentiment\"]].sample(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.30      0.43     44912\n",
      "     Neutral       0.11      0.30      0.16     11652\n",
      "    Positive       0.60      0.72      0.66     44912\n",
      "\n",
      "    accuracy                           0.48    101476\n",
      "   macro avg       0.50      0.44      0.42    101476\n",
      "weighted avg       0.63      0.48      0.50    101476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['Negative', 'Neutral', 'Positive']\n",
    "print(classification_report(df[\"class\"], df[\"Sentiment\"], target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, the LBA model seems to struggle with spelling errors and slang words like \"lit\" which would be out of the lexicon's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bertbase.py\n",
    "In bertbase.py we just take a pretrained model and try to run it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j890098/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get our tokenizer and pretrained model and our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "RANDOM_SEED = 2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment') \n",
    " \n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "df = pd.read_csv('../input/tiktok_google_play_reviews.csv')\n",
    "df = df.dropna(subset='content',axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the 5 star rating into positive negative and neutral sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(rating):\n",
    "    rating = int(rating)\n",
    "    \n",
    "    if rating <= 2:\n",
    "        return -1\n",
    "    elif rating == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Apply to the dataset \n",
    "df['sentiment'] = df.score.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment(review):\n",
    "    tokens = tokenizer.encode(review, return_tensors='pt') \n",
    "    result = model(tokens) \n",
    "    temp = 0 \n",
    "    temp = int(torch.argmax(result.logits))+1 \n",
    "    if temp == 1 or temp == 2: \n",
    "        return -1 \n",
    "    elif temp == 4 or temp == 5: \n",
    "        return 1 \n",
    "    else: \n",
    "        return 0 \n",
    "    \n",
    "class_names = ['Negative', 'Neutral', 'Positive']\n",
    "def to_class(sentiment):\n",
    "    if sentiment == -1:\n",
    "        return 'Negative'\n",
    "    elif sentiment == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cut down the dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:12495]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the sentiment using BERT pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                  content sentiment  \\\n",
      "0                                                   Good  Positive   \n",
      "1      Awesome app! Too many people on it where it's ...  Positive   \n",
      "2                                                Not bad  Positive   \n",
      "3                                             It is good  Negative   \n",
      "4                                   Very interesting app  Positive   \n",
      "...                                                  ...       ...   \n",
      "12490  Very good app I recommend everyone to use this up  Positive   \n",
      "12491                               L think that is well  Positive   \n",
      "12492                             Utter trash for babies  Negative   \n",
      "12493                                        Good please   Neutral   \n",
      "12494                                     Pakistani 💪💪💪💪  Positive   \n",
      "\n",
      "      computed_sentiment  \n",
      "0               Positive  \n",
      "1               Positive  \n",
      "2                Neutral  \n",
      "3               Positive  \n",
      "4               Positive  \n",
      "...                  ...  \n",
      "12490           Positive  \n",
      "12491           Positive  \n",
      "12492           Negative  \n",
      "12493           Positive  \n",
      "12494            Neutral  \n",
      "\n",
      "[12495 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df['computed_sentiment'] = df.content.apply(lambda x: compute_sentiment(x[:MAX_LEN]))\n",
    "df['computed_sentiment'] = df.computed_sentiment.apply(to_class)\n",
    "df['sentiment'] = df.sentiment.apply(to_class)\n",
    "print(df[[\"content\",\"sentiment\",\"computed_sentiment\"]].head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we look how the BERT pretrained model preformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.46      0.51      0.49      1931\n",
      "     Neutral       0.11      0.31      0.16       486\n",
      "    Positive       0.90      0.80      0.85     10078\n",
      "\n",
      "    accuracy                           0.73     12495\n",
      "   macro avg       0.49      0.54      0.50     12495\n",
      "weighted avg       0.80      0.73      0.76     12495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df[\"sentiment\"], df[\"computed_sentiment\"], target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it did very well but for the negative and neutral reviews the F1 score is much worse. This is probably due to the dataset being heavily skewed towards positive reviews so that is fixed in stats.py, and used in reviews_evened.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lexicalBootstrap.py\n",
    "In lexicalBootstrap.py we use the same lexical approach in lexicalBase.py but we implement bootstrapping to alter the lexicon beforehand to include some out of vocabulary words which may help with sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to ../input...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('vader_lexicon',download_dir=\"../input\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "RANDOM_SEED=42\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the lexicon dictionary out of the sentiment analyser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_dict = analyser.make_lex_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenize the reviews and look at how many different tokens we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698880 words total, with a vocabulary size of 43805\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/reviews_evened.csv')\n",
    "\n",
    "df = df.dropna(subset='content',axis=0) \n",
    "class_names = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_descriptions = df_train['content'].apply(tokenizer.tokenize)\n",
    "\n",
    "all_words = [word for tokens in words_descriptions for word in tokens]\n",
    "df_train['description_lengths']= [len(tokens) for tokens in words_descriptions]\n",
    "\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "\n",
    "from collections import Counter\n",
    "count_all_words = Counter(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up converting the polarity of a review to the sentiment of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_score(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    # print(\"{:-<150} {}\".format(sentence, str(score)))\n",
    "sentimentThresh = 0.05\n",
    "def Sentimnt(x):\n",
    "    if x >= sentimentThresh:\n",
    "        return \"positive\"\n",
    "    elif x <= -sentimentThresh:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we hand coded the bootstrapping function following the algorithm listed in the Volkova et al paper which updates the lexicon with extra words and their polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 1\n",
      "num additions to lexicon: 113\n",
      "\n",
      "iteration 2\n",
      "num additions to lexicon: 84\n",
      "\n",
      "iteration 3\n",
      "num additions to lexicon: 110\n",
      "\n",
      "iteration 4\n",
      "num additions to lexicon: 150\n",
      "\n",
      "iteration 5\n",
      "num additions to lexicon: 150\n"
     ]
    }
   ],
   "source": [
    "def defaultval():\n",
    "    return [0,0,0,0,0]\n",
    "def bootstrap():\n",
    "    itercap = 5\n",
    "    falloff = 0.007 #tentative parameters\n",
    "    subjectiveThresh = 0.2\n",
    "    minCount = 50\n",
    "    numAdditionsPerIter = 150\n",
    "\n",
    "    iter = 0\n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        additions_to_lex = {}\n",
    "        unknownwords = set(all_words) - set(lex_dict)\n",
    "        unkWordDict = defaultdict(defaultval)# [key] = [subjectivity prob, positivity prob, subjectivity count, positivity count, count of appearances]\n",
    "        unkWordDict.clear()\n",
    "        sdict = {}\n",
    "        sdict.clear()\n",
    "        print(\"\\niteration\",iter+1)\n",
    "        # print(\"num of unknown words:\",len(unknownwords))\n",
    "        for unk in unknownwords:\n",
    "            if count_all_words[unk] > minCount:\n",
    "                for sentence in words_descriptions:\n",
    "                    if unk in sentence:\n",
    "                        for word in sentence:\n",
    "                            subjective = False\n",
    "                            # positive = False\n",
    "                            if word in lex_dict:\n",
    "                                # if lex_dict[word] > 0:\n",
    "                                #     positive = True\n",
    "                                subjective = True\n",
    "                        if subjective == True:\n",
    "                            if (analyser.polarity_scores(''.join(sentence)))['compound']>sentimentThresh: #if the analyser says the sentence is positive\n",
    "                                unkWordDict[unk][3] = unkWordDict[unk][3] + 1   #mark the positivity count\n",
    "                            unkWordDict[unk][2] = unkWordDict[unk][2] + 1\n",
    "                            \n",
    "                        unkWordDict[unk][4] = unkWordDict[unk][4] + 1   #mark the count of appearances\n",
    "                unkWordDict[unk][0] = unkWordDict[unk][2] / unkWordDict[unk][4] - iter*falloff #subjectivity prob\n",
    "                if(unkWordDict[unk][2] == 0):\n",
    "                    unkWordDict[unk][1]=0\n",
    "                else:\n",
    "                    unkWordDict[unk][1] = unkWordDict[unk][3] / unkWordDict[unk][2] #positivity prob\n",
    "        sdict = dict(sorted(unkWordDict.items(), key=lambda item: item[1][0]))   #sort by subjectivity\n",
    "        k = 0\n",
    "        \n",
    "        for item in reversed(sdict.items()):    #iterate through decreasing subjectivity\n",
    "            # print(item)\n",
    "            if item[1][0] > subjectiveThresh:   #check against the minimum subjectivity allowed\n",
    "                if item[1][1] > 0.5:\n",
    "                    additions_to_lex[item[0]] = 1   #set the polarity\n",
    "                else:\n",
    "                    additions_to_lex[item[0]] = -1\n",
    "            k = k+1\n",
    "            if k >= numAdditionsPerIter:\n",
    "                # print(\"last subjectivity:\",item[1][0])\n",
    "                break\n",
    "                                    \n",
    "        iter = iter +1\n",
    "        print(\"num additions to lexicon:\", len(additions_to_lex))\n",
    "        if len(additions_to_lex) == 0 or iter>=itercap:\n",
    "            stop=True\n",
    "        else:\n",
    "            lex_dict.update(additions_to_lex)\n",
    "            analyser.lexicon.update(additions_to_lex)\n",
    "\n",
    "bootstrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run sentiment analysis on the data with our new bootstrapped lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['scores'] = df_test['content'].apply(lambda review: analyser.polarity_scores(review))\n",
    "df_test['compound']  = df_test['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "df_test['Sentiment'] = df_test['compound'].apply(Sentimnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content     class Sentiment\n",
      "52712                                      Vast nice app  negative  positive\n",
      "16529  Is your app not compatible with Android anymor...  negative  negative\n",
      "57186                                     Bts funny pics  negative  positive\n",
      "57991                                          Goodd app  negative   neutral\n",
      "48440                                                  👍   neutral   neutral\n",
      "97507                               I love tikto so much  positive  positive\n",
      "83749  If they don't fix the problem I will delete th...  negative  negative\n",
      "67494                                  Gander change app  negative   neutral\n",
      "87470                                    Tiktok is great  positive  positive\n",
      "85879  I love it but i put the wrong birth date and n...  negative  negative\n",
      "12750                                            K death  negative  negative\n",
      "60244                                    For you problem  negative  negative\n",
      "86215                        Viral my videos Please 😭😭😭😭   neutral  positive\n",
      "77646  My live is not working I really want to watch ...  negative  negative\n",
      "7167                                                Good  positive  positive\n",
      "22138  Hey tiktok team I have 187k followers and 3.3m...  negative  negative\n",
      "49056                                               Good  positive  positive\n",
      "90886  I don't need the piece of shirt popping up eve...  negative  negative\n",
      "49318  Sir mera app y Update nahi horha Iski salution...  negative  negative\n",
      "72611                                Thank you very much  positive  positive\n"
     ]
    }
   ],
   "source": [
    "print(df_test[[\"content\",\"class\",\"Sentiment\"]].sample(n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with bootstrapping the LBA model hase some trouble with all the misspelled words and out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.63      0.52      0.57     13421\n",
      "     Neutral       0.10      0.21      0.14      3434\n",
      "    Positive       0.67      0.61      0.64     13588\n",
      "\n",
      "    accuracy                           0.53     30443\n",
      "   macro avg       0.47      0.45      0.45     30443\n",
      "weighted avg       0.59      0.53      0.55     30443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test[\"class\"], df_test[\"Sentiment\"], target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBA with bootstrapping did better than it's base LBA model however it is still behind the BERT model's F1 scores and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bertTraining.py\n",
    "In this file we start with a pretrained BERT model but then do further finetuning on our dataset. The base BERT model already outpreformed our LBA with bootstrapping but the information provided from this model will still be useful and interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# Torch ML libraries\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Misc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set intial variables and constants\n",
    "\n",
    "# Random seed for reproducibilty\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Set GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\") # or force torch to run on cpu\n",
    "\n",
    "df = pd.read_csv('../input/reviews_evened.csv')\n",
    "# df = df[:1249] #cut dataset down for testing\n",
    "df = df[:12495] #cut dataset down for testing\n",
    "\n",
    "df = df.dropna(subset='content',axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same kind of setup as the base BERT, and check the count for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[586, 137, 526]\n"
     ]
    }
   ],
   "source": [
    "sentimentcount = [0,0,0]\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "def to_sentiment(rating):\n",
    "    rating = int(rating)\n",
    "    # Convert to class\n",
    "    if rating <= 2:\n",
    "        sentimentcount[0] = sentimentcount[0]+1\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        sentimentcount[1] = sentimentcount[1]+1\n",
    "        return 1\n",
    "    else:\n",
    "        sentimentcount[2] = sentimentcount[2]+1\n",
    "        return 2\n",
    "\n",
    "# Apply to the dataset \n",
    "df['sentiment'] = df.score.apply(to_sentiment)\n",
    "print(sentimentcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then retrieve the pretrained model and tokenize out reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Store length of each review \n",
    "token_lens = []\n",
    "\n",
    "# Iterate through the content slide\n",
    "for txt in df.content:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the data is set up and split into test, development, and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 160\n",
    "\n",
    "class GPReviewDataset(Dataset):\n",
    "    # Constructor Function \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # Length magic method\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    # get item magic method\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        # Encoded format to be returned \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.content.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "BATCH_SIZE = 16\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "data = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the model is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    # Constructor class \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    # Forward propagaion class\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          return_dict=False\n",
    "        )\n",
    "        #  Add a dropout layer \n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "    \n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are then defined, we did not change these from the default as the BERT baseline already outpreformed our lexical based approach with bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations \n",
    "EPOCHS = 10      \n",
    "\n",
    "# Optimizer Adam \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Set the loss function \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training model is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Descent\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the BERT model with training is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.9611749941652471 accuracy 0.528604118993135\n",
      "Val   loss 0.9210618187983831 accuracy 0.5668449197860962\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.8667353136972947 accuracy 0.6361556064073226\n",
      "Val   loss 0.9803924212853113 accuracy 0.5294117647058824\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.7023561079393733 accuracy 0.7219679633867276\n",
      "Val   loss 0.9925762762626013 accuracy 0.5775401069518716\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.5516707528721202 accuracy 0.7940503432494279\n",
      "Val   loss 1.1979134728511174 accuracy 0.6042780748663101\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.39144366519017654 accuracy 0.8672768878718535\n",
      "Val   loss 1.4673255781332653 accuracy 0.5828877005347594\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.2904203066094355 accuracy 0.8981693363844394\n",
      "Val   loss 1.8361041347185771 accuracy 0.53475935828877\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.2718613896010952 accuracy 0.8993135011441648\n",
      "Val   loss 1.6461984813213348 accuracy 0.5668449197860962\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.2274026376957243 accuracy 0.9210526315789473\n",
      "Val   loss 1.7682879666487377 accuracy 0.5828877005347594\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.20189064001156526 accuracy 0.9267734553775744\n",
      "Val   loss 1.80378125111262 accuracy 0.5828877005347594\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.18790015419098463 accuracy 0.9324942791762014\n",
      "Val   loss 1.836022545893987 accuracy 0.5775401069518716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Show details \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "    \n",
    "    # Get model performance (accuracy and loss)\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    \n",
    "    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # If we beat prev performance\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the full dataset, running on a Nvidia RTX 3070ti the training takes more than 4 hours to complete. After waiting we then can get the evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            # Get outouts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "    return review_texts, predictions, prediction_probs, real_values\n",
    "\n",
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out some of the results looking at the real class and comparing with the computed sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               content     class sentiment\n",
      "165                                                 ✌️  Positive  Positive\n",
      "90                                                 😒😒😒  Negative  Positive\n",
      "64   I love tiktok @rabin___officail please support...  Positive  Positive\n",
      "171                 Wow so cute dxt plz bro support me  Negative  Positive\n",
      "122                                           Good app  Negative  Positive\n",
      "51                                                 ❤❤❤   Neutral  Positive\n",
      "187                                     i love tik tok  Positive  Positive\n",
      "52                                               Great  Positive  Positive\n",
      "145  I love posting my own content that I enjoy, it...  Positive   Neutral\n",
      "76   It's a good app... but everytime I change my p...  Positive   Neutral\n",
      "80                                            Best app  Positive  Positive\n",
      "66                                            বাংলাদেশ  Positive  Positive\n",
      "7                               Tik tok is the best???  Positive  Negative\n",
      "118  Enchanting experience, great opportunities to ...  Positive  Positive\n",
      "39                                           Legit app  Positive  Negative\n",
      "40   A stupid security bug that allows hackers to g...  Negative  Negative\n",
      "137                                             I am d   Neutral  Negative\n",
      "116               Great place to meet and make friends  Positive  Positive\n",
      "50   It's good but I have got so many weird things ...  Positive  Positive\n",
      "35                                     It is very good  Positive  Positive\n"
     ]
    }
   ],
   "source": [
    "px[\"content\"] = pd.DataFrame(y_review_texts)\n",
    "px[\"class\"] = pd.DataFrame(y_test)\n",
    "px[\"sentiment\"] = pd.DataFrame(y_pred)\n",
    "def to_class(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return 'Negative'\n",
    "    elif sentiment == 1:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "px['class'] = px[\"class\"].apply(to_class)\n",
    "px['sentiment'] = px[\"sentiment\"].apply(to_class)\n",
    "print(px[[\"content\",\"class\",\"sentiment\"]].sample(n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results appear much better than the LBA model, some of the emojis have been classified instead of just being neutral for all emojis. We can also get the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.66      0.65        92\n",
      "     neutral       0.29      0.25      0.27        20\n",
      "    positive       0.64      0.63      0.64        76\n",
      "\n",
      "    accuracy                           0.61       188\n",
      "   macro avg       0.52      0.51      0.52       188\n",
      "weighted avg       0.60      0.61      0.60       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model with finetuning preforms better than just the pretrained model by itself when they are both using the same dataset, but both preform much better on the original skewed dataset. This may be because the BERT model is able to assign bias weight values to each of the classes which allows it to have a much better accuracy when the data is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "Both of the BERT models did much better at the classification task than either of the LBA models. The very challenging dataset full of typos and slang seemed to affect both models significantly, but especially affected the LBA models. In the LBA, out of vocabulary words are just given neutral values and that is all. With bootstrapping, the lexicon is able to pick up a few out of vocabulary words and add them to the lexicon but this is not enough to match either of the BERT tests. The BERT model is able to look at the context surrounding the out of vocabulary words to gain information about it, but the BERT model does not even have an explicit vocabulary in the first place which only helps BERT for this dataset. Finetuning on the dataset does help the accuracy of the BERT model, but the final accuracy is still far behind the normal results when tested on a \"clean\" dataset. This is expected that both approaches run worse on the very flawed dataset and we found that BERT is much better at adapting to this dataset than LBA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4499c17f0daef8f4cda6a68fb34a7495dc18f3df622959f4dd64a5640a9440f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
